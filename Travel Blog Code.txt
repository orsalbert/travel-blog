BASH

  #replacing the white space with semicolon between date and time in each row
sed 's/ /;/' travelblog.csv > travels.csv

  #separating rows based on ‘read’, ‘buy’, ‘subscribe’ criterias
grep 'read' travels.csv | sort -V > read.csv
grep 'buy' travels.csv | sort -V > buy.csv
grep 'subscribe' travels.csv | sort -V > subscribe.csv

  #creating .csv file with first time visitors
grep 'AdWords' read.csv >> temp.csv
grep ‘SEO’ read.csv >> temp.csv
grep ‘Reddit’ read.csv >> temp.csv

sort -V temp.csv > first_read.csv

rm temp.csv

  #creating .csv file with recurring visitors
grep -v 'AdWords' read.csv | grep -v 'Reddit' | grep -v 'SEO' > recurring.csv


SQL

  --create table for new and for recurring visitors, purchases and subscriptions

CREATE TABLE first_read (
  my_date DATE,
  my_time TIME,
  event_type TEXT,
  country TEXT,
  user_id BIGINT,
  source TEXT,
  topic TEXT);

COPY first_read FROM ‘~/travelblog/first_read.csv' DELIMITER ';';

CREATE TABLE recurring (
  my_date DATE,
  my_time TIME,
  event_type TEXT,
  country TEXT,
  user_id BIGINT,
  topic TEXT);
  
COPY recurring FROM ‘~/travelblog/recurring.csv' DELIMITER ';';

CREATE TABLE buy (
  my_date DATE,
  my_time TIME,
  event_type TEXT,
  user_id BIGINT,
  amount INT);
  
COPY buy FROM ‘~/travelblog/buy.csv' DELIMITER ';';

CREATE TABLE subscribe (
  my_date DATE,
  my_time TIME,
  event_type TEXT,
  user_id BIGINT);
  
COPY subscribe FROM ‘~/travelblog/subscribe.csv' DELIMITER ';';

 --create table for all readers that visit the blog (fresh visitors + recurring)

CREATE TABLE all_read AS
SELECT big_table.my_date,
       big_table.event_type,
       big_table.country,
       big_table.user_id,
       fr_source.source,
       big_table.topic
FROM
(SELECT * FROM
  (SELECT my_date,
          my_time,
          event_type,
          country,
          user_id,
          topic 
  FROM first_read) AS fr
UNION ALL
  (SELECT *
  FROM recurring)) AS big_table 
JOIN (
   SELECT user_id,
          source
   FROM first_read) AS fr_source
ON fr_source.user_id = big_table.user_id
ORDER BY big_table.user_id;

  --all_read SEGMENTS: by topic, by source, by country

SELECT topic, COUNT(*) c
FROM all_read
GROUP BY topic
ORDER BY c DESC;

SELECT source, COUNT(*) c
FROM all_read
GROUP BY source
ORDER BY c DESC;

SELECT country, COUNT(*) c
FROM all_read
GROUP BY country
ORDER BY c DESC;

  --first_read SEGMENTS by source, by country, by topic

SELECT source,
       COUNT(*) AS c
FROM first_read
GROUP BY source
ORDER BY c DESC;

SELECT country,
       COUNT(*) AS c
FROM first_read
GROUP BY country
ORDER BY c DESC;

SELECT topic,
       COUNT(*) AS c
FROM first_read
GROUP BY topic
ORDER BY c DESC;

    --microsegment    
SELECT source,
       topic,
       country,
       COUNT(*) as c
FROM first_read
GROUP BY source,
         topic,
         country
ORDER BY c DESC;

  --purchases table

    --total number of purchases
SELECT COUNT(*)
FROM buy;

    --total number of purchases per product
SELECT amount, COUNT(*)
FROM buy
GROUP BY amount;

    --daily revenue
SELECT my_date, SUM(amount)
FROM buy
GROUP BY my_date
ORDER BY my_date;

    --total revenue
SELECT SUM(amount) as REV
FROM buy;

    -- daily revenue by product
SELECT my_date,
       amount,
       COUNT(*),
       amount*COUNT(*) AS daily_rev
FROM buy
GROUP BY my_date,
         amount
ORDER BY my_date;

  --subscription table

SELECT COUNT(*)
FROM subscribe;

SELECT my_date,
       COUNT(*)
FROM subscribe
GROUP BY my_date
ORDER BY my_date;

    --which source brought the most subscribers?

SELECT source, COUNT(*) c
FROM first_read fr
JOIN subscribe sub
  ON fr.user_id = sub.user_id
GROUP BY source
ORDER BY c DESC;

    --DATA STUDIO: daily subscribers by source

SELECT sub.my_date, source, COUNT(*)
FROM first_read fr
JOIN subscribe sub
  ON fr.user_id = sub.user_id
GROUP BY sub.my_date, source;

  --which source brought the most revenue?

SELECT source, SUM(amount) rev
FROM first_read fr
JOIN buy 
  ON fr.user_id = buy.user_id
GROUP BY source
ORDER BY rev DESC;

    --DATA STUDIO: revenue by segments

SELECT source, topic, SUM(amount) rev
FROM (
      SELECT buy.my_date, buy.user_id, fr.source, topic, amount
      FROM first_read fr
      JOIN buy 
        ON fr.user_id = buy.user_id) AS subq
GROUP BY source, topic
ORDER BY rev DESC;

  --Daily active users (number of distinct user_ids per day)

SELECT my_date, COUNT(DISTINCT(user_id))
FROM all_read
GROUP BY my_date
ORDER BY my_date;

  --Daily active users including subs and purchase

SELECT my_date,
       COUNT(DISTINCT(user_id))
FROM (
      SELECT my_date, user_id FROM all_read
      UNION ALL
      SELECT my_date, user_id FROM subscribe
      UNION ALL
      SELECT my_date, user_id FROM buy
      ORDER BY my_date) AS subq
GROUP BY my_date;

  --Daily new visitors

SELECT my_date, COUNT(user_id)
FROM first_read
GROUP BY my_date
ORDER BY my_date;

  --Daily new subscriptions

SELECT my_date, COUNT(user_id)
FROM subscribe
GROUP BY my_date
ORDER BY my_date;

  --Daily acquisitions

SELECT my_date, COUNT(user_id)
FROM buy
GROUP BY my_date
ORDER BY my_date;

  --FUNNEL

SELECT visit.my_date,
       visit.country,
       visit.source,
       visit.visitors,
       ret.return_reads,
       subs.subscribers,
       cust.customers
FROM 
    -- FUNNEL #1 new readers
    (SELECT my_date,
           country,
           source,
           COUNT(*) visitors
    FROM first_read
    GROUP BY my_date,
             country,
             source
    ORDER BY my_date) AS visit
LEFT JOIN
    -- FUNNEL #2 returning readers
    (SELECT fr.my_date,
            fr.country,
            fr.source,
            COUNT(DISTINCT(rec.user_id)) return_reads
    FROM first_read fr
    RIGHT JOIN recurring rec
    ON fr.user_id = rec.user_id
    GROUP BY fr.my_date,
             fr.country,
             fr.source
    ORDER BY fr.my_date) AS ret
ON visit.my_date = ret.my_date
   AND visit.country = ret.country
   AND visit.source = ret.source
LEFT JOIN
    -- FUNNEL #3 subscribers
    (SELECT fr.my_date,
           fr.country,
           fr.source,
           COUNT(*) subscribers
    FROM first_read fr
    JOIN subscribe sub
      ON fr.user_id = sub.user_id
    GROUP BY fr.my_date,
             fr.country,
             fr.source
    ORDER BY my_date) AS subs
ON visit.my_date = subs.my_date
   AND visit.country = subs.country
   AND visit.source = subs.source
LEFT JOIN
    -- FUNNEL #4 customers
    (SELECT fr.my_date,
           fr.country,
           fr.source,
           COUNT(DISTINCT(buy.user_id)) customers
    FROM first_read fr
    JOIN buy
      ON fr.user_id = buy.user_id
    GROUP BY fr.my_date,
             fr.country,
             fr.source
    ORDER BY my_date) AS cust
ON visit.my_date = cust.my_date
   AND visit.country = cust.country
   AND visit.source = cust.source;

--last 7 days average reads

SELECT AVG(c)
FROM (
      SELECT my_date, COUNT(*) c
      FROM all_read
      WHERE my_date BETWEEN '2018-03-24' AND '2018-03-30'
      GROUP BY my_date) AS subq;

PYTHON

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt 
%matplotlib inline

first_read = pd.read_csv(‘~/travelblog/first_read.csv', delimiter = ';', names = ['my_date', 'my_time', 'event_type', 'country', 'user_id', 'source', 'topic'])

#Which source brought the most visitors?

first_read.groupby('source').count()[['user_id']].sort_values(by = 'user_id', ascending = False)

#What source brought the most purchases?

purchases = pd.read_csv(‘~/travelblog/buy.csv', delimiter = ';', names = ['my_date', 'my_time', 'event_type', 'user_id', 'amount'])
visitors_purchases = first_read.merge(purchases, on = 'user_id')
visitors_purchases.groupby('source').sum()[['amount']].sort_values(by = 'amount', ascending = False)

visitors_purchases.groupby('source').count()[['user_id']].sort_values(by = 'user_id', ascending = False)

#Visualize the number of daily new visitors

viz = first_read.groupby('my_date').count()[['event_type']]
viz.plot()

#What is the projected number of reads by the end of next month?

source_list = first_read[['user_id', 'source']]

recurring = pd.read_csv(‘~/travelblog/recurring.csv', delimiter = ';', names = ['my_date', 'my_time', 'event_type', 'country', 'user_id', 'topic'])

recurring_source = recurring.merge(source_list, on = 'user_id').sort_values(by = 'my_date').reset_index(drop = True)

all_reads = pd.concat([first_read, recurring_source])

all_reads = all_reads.sort_values(by = 'my_date')

reads_filt = all_reads[all_reads['my_date'] != '2018-03-31']

daily_reads = reads_filt.groupby('my_date').count()

daily_reads.plot()

formatted = daily_reads.reset_index(drop = True).event_type

x = formatted.index
y = formatted.values

coefs = np.polyfit(x, y, 2)
predict = np.poly1d(coefs)

from sklearn.metrics import r2_score
r2_score(y, predict(x))
##0.7354429847316264

first_read[['my_date']].nunique()

x_test = np.linspace(0,120)
y_pred = predict(x_test[:, None])
plt.scatter(x, y, c = 'darkcyan')
plt.plot(x_test, y_pred, c = 'r')
plt.show()

predict(88+30)
##14131.093642170821

#Revenue projection

purchases = pd.read_csv(‘~/travelblog/buy.csv', delimiter = ';', names = ['my_date', 'my_time', 'event_type', 'user_id', 'amount'])
purchases.head()

formatted = purchases[['my_date', 'amount']].groupby('my_date').sum().reset_index(drop = True).amount
viz.plot()

x = formatted.index
y = formatted.values

coefs = np.polyfit(x, y, 1)
predict = np.poly1d(coefs)

from sklearn.metrics import r2_score
r2_score(y, predict(x))
##0.5783472575365922

purchases[['my_date']].nunique()

x_test = np.linspace(0,89+30)
y_pred = predict(x_test[:, None])
plt.scatter(x, y, c = 'darkcyan')
plt.plot(x_test, y_pred, c = 'r')
plt.show()

predict(89+30)
##5776.7824310520955

#COHORT Analysis

import csv

date_list = []
with open ('first_read.csv') as csvfile:
    readCSV = csv.reader(csvfile, delimiter = ';')
    for row in readCSV:
        if row[0] not in date_list:
            date_list.append(row[0])

user_reads = {}
with open ('all_reads.csv') as csvfile:
    readCSV = csv.reader(csvfile, delimiter = ';')
    for row in readCSV:
        if row[4] not in user_reads:
            user_reads[row[4]] = []
        if row[0] not in user_reads[row[4]]:
            user_reads[row[4]].append(row[0])

user_reads_list = list(user_reads.values())

#COHORT SCRIPT
print("cohort;days_passed;number_of_users")
for cohort in date_list:
    day_passed = 0
    for given_date in date_list[date_list.index(cohort):date_list.index(cohort)+8]:
        number_of_users = 0
        for users in user_reads_list:
            if given_date in users and users[0] == cohort:
                number_of_users = number_of_users + 1
        print(f"{cohort};{day_passed};{number_of_users}")
        day_passed = day_passed + 1